{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98eea061",
   "metadata": {},
   "source": [
    "## ðŸ§© Step 1 â€” Import Required Libraries\n",
    "This section imports all the necessary Python libraries used throughout the project.  \n",
    "It includes:\n",
    "- Core Python libraries for file handling  \n",
    "- Data analysis and visualization tools  \n",
    "- PyTorch and torchvision for deep learning  \n",
    "- Warnings handling to keep logs clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806f459b",
   "metadata": {},
   "source": [
    "##  Environment Setup (for all collaborators)\n",
    "\n",
    "Before running this notebook, make sure that **PyTorch** and related libraries are installed.  \n",
    "Weâ€™re using the **CPU version** (no GPU required) to ensure compatibility for everyone.\n",
    "\n",
    "###  Installation command (run this only once)\n",
    "\n",
    "```bash\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc4c8022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "#  Kaoutar : Setup and  Imports \n",
    "\n",
    "# Core\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Image handling\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\" Libraries imported successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9846c516",
   "metadata": {},
   "source": [
    "##  Step 2 â€” Data Loading & Cleaning\n",
    "\n",
    "In this section, we will:\n",
    "- Load image paths from the **raw dataset**\n",
    "- Verify that each image has a valid extension (`.jpg`, `.jpeg`, `.bmp`, `.png`)\n",
    "- Handle loading errors with a `try/except` block\n",
    "- Save successfully verified images into the directory: `data/cleaned/`\n",
    "\n",
    "This ensures that only valid and readable images are used for training, avoiding issues later during model loading.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb255bb",
   "metadata": {},
   "source": [
    "### Dataset Instructions\n",
    "\n",
    "Download the dataset for this project (Blood_Cells_Cancer),  \n",
    "extract it, and place it in:\n",
    "\n",
    "data/raw/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2e48dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Raw data path: ..\\data\\raw\\Blood_Cells_Cancer\\Blood cell Cancer [ALL]\n",
      " Cleaned data path: ..\\data\\cleaned\n",
      " Folder structure ready.\n"
     ]
    }
   ],
   "source": [
    "#  Kaoutar : Dataset Loading and  Cleaning\n",
    "\n",
    "# Define input raw and output (cleaned) directories\n",
    "raw_data_path = Path(\"../data/raw/Blood_Cells_Cancer/Blood cell Cancer [ALL]\")\n",
    "cleaned_data_path = Path(\"../data/cleaned\")\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "cleaned_data_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Valid image extensions\n",
    "valid_extensions = {\".jpg\", \".jpeg\", \".png\", \".bmp\"}\n",
    "\n",
    "print(\" Raw data path:\", raw_data_path)\n",
    "print(\" Cleaned data path:\", cleaned_data_path)\n",
    "print(\" Folder structure ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb91035",
   "metadata": {},
   "source": [
    "###  Clean and Copy Valid Images\n",
    "\n",
    "Below, we loop through each class folder (each represents a cancer cell type).  \n",
    "For each image:\n",
    "- We check if the file extension is valid  \n",
    "- We attempt to open it with PIL (inside a `try/except` block)  \n",
    "- If the image opens successfully, we copy it into the `data/cleaned` directory  \n",
    "- Otherwise, we skip it and print a warning message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5213d68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing class: Benign\n",
      " Processing class: early Pre-B\n",
      " Processing class: Pre-B\n",
      " Processing class: Pro-B\n",
      " Dataset cleaning complete. Cleaned data stored in /data/cleaned/\n"
     ]
    }
   ],
   "source": [
    "# Loop through each class folder in the raw dataset\n",
    "for class_name in os.listdir(raw_data_path):\n",
    "    class_path = raw_data_path / class_name\n",
    "    \n",
    "    # Skip non-directory files\n",
    "    if not class_path.is_dir():\n",
    "        continue\n",
    "    \n",
    "    # Create corresponding class folder in cleaned dataset\n",
    "    target_class_path = cleaned_data_path / class_name\n",
    "    target_class_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\" Processing class: {class_name}\")\n",
    "    \n",
    "    for image_name in os.listdir(class_path):\n",
    "        image_path = class_path / image_name\n",
    "        ext = image_path.suffix.lower()\n",
    "        \n",
    "        # Skip invalid extensions\n",
    "        if ext not in valid_extensions:\n",
    "            print(f\" Skipping invalid file: {image_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Try to open the image\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                img.verify()  # verify ensures the file is not corrupted\n",
    "                \n",
    "            # Copy valid image to cleaned folder\n",
    "            shutil.copy(image_path, target_class_path)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\" Error loading {image_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "print(\" Dataset cleaning complete. Cleaned data stored in /data/cleaned/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edef054",
   "metadata": {},
   "source": [
    "##  Step 3 â€” Split Dataset into Train / Validation / Test\n",
    "\n",
    "We will split the cleaned dataset into three sets:\n",
    "- **Train:** 70%\n",
    "- **Validation:** 15%\n",
    "- **Test:** 15%\n",
    "\n",
    "The structure will be preserved for each class, and images will be copied to:\n",
    "\n",
    "data/splits/train/  \n",
    "data/splits/val/  \n",
    "data/splits/test/\n",
    "\n",
    "This allows the next notebook to directly use these splits for exploration, augmentation, and model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51120acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dataset split complete!\n"
     ]
    }
   ],
   "source": [
    "#  Kaoutar : Split Dataset \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Paths\n",
    "cleaned_data_path = Path(\"../data/cleaned\")\n",
    "splits_path = Path(\"../data/splits\")\n",
    "train_path = splits_path / \"train\"\n",
    "val_path = splits_path / \"val\"\n",
    "test_path = splits_path / \"test\"\n",
    "\n",
    "# Create directories\n",
    "for path in [train_path, val_path, test_path]:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Split dataset class by class\n",
    "for class_name in os.listdir(cleaned_data_path):\n",
    "    class_path = cleaned_data_path / class_name\n",
    "    images = [f for f in os.listdir(class_path) if (class_path / f).is_file()]\n",
    "    \n",
    "    # 70/30 split first\n",
    "    train_imgs, temp_imgs = train_test_split(images, test_size=0.3, random_state=42)\n",
    "    # Then split temp into validation and test 50/50 -> 15% each\n",
    "    val_imgs, test_imgs = train_test_split(temp_imgs, test_size=0.5, random_state=42)\n",
    "    \n",
    "    # Function to copy images\n",
    "    def copy_images(img_list, target_dir):\n",
    "        target_class_dir = target_dir / class_name\n",
    "        target_class_dir.mkdir(parents=True, exist_ok=True)\n",
    "        for img in img_list:\n",
    "            shutil.copy(class_path / img, target_class_dir)\n",
    "    \n",
    "    # Copy to respective folders\n",
    "    copy_images(train_imgs, train_path)\n",
    "    copy_images(val_imgs, val_path)\n",
    "    copy_images(test_imgs, test_path)\n",
    "\n",
    "print(\" Dataset split complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99264640",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02b3cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Kaoutar â€” Load Pretrained GoogLeNet\n",
    "\n",
    "# Number of classes (adjust based on your dataset)\n",
    "num_classes = len(os.listdir(\"../data/cleaned\"))\n",
    "\n",
    "# Load pretrained GoogLeNet\n",
    "googlenet = models.googlenet(pretrained=True)\n",
    "\n",
    "# Replace the fully connected (FC) layer\n",
    "# GoogLeNet's FC layer is named 'fc'\n",
    "googlenet.fc = nn.Sequential(\n",
    "    nn.Linear(googlenet.fc.in_features, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(512, num_classes)\n",
    ")\n",
    "\n",
    "print(\" GoogLeNet loaded and FC layer replaced.\")\n",
    "print(googlenet)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
